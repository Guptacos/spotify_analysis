{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this notebook, we will be reading from a pickle file, which will contain a list of songs through the years. Each element of this list will be a year of top songs, which is a list of dicts containing date, name, and artist. This list should contain the Billboard 100 songs for every week for the last 20 years, or as many years are available for that genre.\n",
    "\n",
    "Given this list, we will query the spotify API to generate a dataframe that contains the same information, augmented with song ID, song features, and artist genre. This dataframe will be saved as a pickle file at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "import spotipy.util as util\n",
    "\n",
    "scope = ''\n",
    "username = 'testname777123'\n",
    "# Note: This should be filled in with an authentication token. Make sure to delete it before committing.\n",
    "token = 'BQAJPxMVHIE-xyglkb03mO7cTv0XO8yPn6WcZReTGih7kNhBLwTqiXb0VsTDx9Z9_hWN0fz4xwCD7tq_EMBUsI3UuM4Dj1LhV_9ltfmnxBAFc7cVtGl77Pa7Vh39GgJa3b9g6bH4i4kx-Noyu1_KfK7t'\n",
    "\n",
    "sp = spotipy.Spotify(auth=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a cache of song mappings so as to limit our API requests to spotify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format is dict{artist : {song : data, song : data ...} ...}\n",
    "memo = dict()\n",
    "unique_unfound = 0\n",
    "total_unfound = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's devise a way to get a song id, given a song and an artist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "TITLE_DIFF = 70\n",
    "ARTIST_DIFF = 70\n",
    "\n",
    "def contains_artist(goal, artists):\n",
    "    # Use unidecode to turn special characters to regular, i.e. Ã± --> n\n",
    "    goal = unidecode(goal).lower()\n",
    "    \n",
    "    for artist in artists:\n",
    "        art = unidecode(artist['name']).lower()\n",
    "        if fuzz.partial_ratio(art, goal) >= ARTIST_DIFF:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# If the song is not in the top 20 results, ignore it.\n",
    "# TOOD: how frequently does this happen? If frequently, maybe follow the pages?\n",
    "def get_song(track, artist):\n",
    "    global unique_unfound\n",
    "    track = unidecode(track).lower()\n",
    "    artist = unidecode(artist).lower()\n",
    "    \n",
    "    extra_search_words = {'is', 'a', 'i', 'the'}\n",
    "    \n",
    "    split1 = track.split()\n",
    "    first = split1[0]\n",
    "    if split1[0] in extra_search_words and len(split1) > 1:\n",
    "        first += ' ' + split1[1]\n",
    "    \n",
    "    split2 = artist.split()\n",
    "    second = split2[0]\n",
    "    if split2[0] in extra_search_words and len(split2) > 1:\n",
    "        second += ' ' + split2[1]\n",
    "        \n",
    "    to_search = first + ' ' + second\n",
    "    query_result = sp.search(q=to_search, type='track', limit=40)\n",
    "    query_result = query_result['tracks']['items']\n",
    "    \n",
    "    # Clean the query results\n",
    "    result = []\n",
    "    for elem in query_result:\n",
    "        cur_name = unidecode(elem['name']).lower()\n",
    "        \"\"\"\n",
    "        arts = []\n",
    "        for a in elem['artists']:\n",
    "            arts += [a['name']]\n",
    "        print(elem['name'], arts, fuzz.partial_ratio(track, cur_name), contains_artist(artist, elem['artists']))\n",
    "        \"\"\"\n",
    "        if fuzz.partial_ratio(track, cur_name) < TITLE_DIFF:\n",
    "            continue\n",
    "        if not contains_artist(artist, elem['artists']):\n",
    "            continue\n",
    "\n",
    "        new_song = dict()\n",
    "        new_song['name'] = cur_name\n",
    "        new_song['id'] = elem['id']\n",
    "        new_song['artist'] = artist\n",
    "        new_song['popularity'] = elem['popularity']\n",
    "        \n",
    "        result += [new_song]\n",
    "    \n",
    "    # Return the most popular song\n",
    "    result.sort(key=lambda d: d['popularity'])\n",
    "    \n",
    "    if len(result) == 0:\n",
    "        print(\"Could not find '%s' by '%s' in top 20 results\" % (track, artist))\n",
    "        unique_unfound += 1\n",
    "        return None\n",
    "    \n",
    "    return result[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a wrapper to do it given a list of songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "CHUNK_SIZE = 50\n",
    "\n",
    "def divide_chunks(l, n):\n",
    "    result = []\n",
    "    for i in range(0, len(l), n):  \n",
    "        if i >= len(l):\n",
    "            result += [l[i:]]\n",
    "        else:\n",
    "            result += [l[i:i + n]]\n",
    "    return result\n",
    "\n",
    "def del_unneeded(d):\n",
    "    del d['type']\n",
    "    del d['id']\n",
    "    del d['uri']\n",
    "    del d['track_href']\n",
    "    del d['analysis_url']\n",
    "    #TODO: do we want time signature? Do we want to remove duration?\n",
    "    del d['time_signature']\n",
    "    \n",
    "    return d\n",
    "\n",
    "def combine_fn(a, b):\n",
    "    a.update(b)\n",
    "    return a\n",
    "\n",
    "#TODO: should integrate 'retry_after' when response 429, to deal with rate limiting\n",
    "# Expects song_list of type [dict{week, name, artist}....]\n",
    "def convert_all_songs(song_list):\n",
    "    global unique_unfound\n",
    "    global total_unfound\n",
    "    \n",
    "    unique_unfound = 0\n",
    "    total_unfound = 0\n",
    "    \n",
    "    result_data = []\n",
    "    for song in song_list:\n",
    "        artist = unidecode(song['artist']).lower()\n",
    "        title = unidecode(song['name']).lower()\n",
    "        if artist not in memo:\n",
    "            memo[artist] = dict()\n",
    "        \n",
    "        # Check if we've already hit the API for this song\n",
    "        if title in memo[artist]:\n",
    "            temp = copy.copy(memo[artist][title])\n",
    "        else:\n",
    "            # Note: spotipy takes care of the 429 'retry-after' response from spotify\n",
    "            temp = get_song(title, artist)\n",
    "            \n",
    "        if temp is not None:\n",
    "            temp['date'] = song['date']\n",
    "            result_data += [temp]\n",
    "        else:\n",
    "            total_unfound += 1\n",
    "        \n",
    "        memo[artist][title] = temp\n",
    "\n",
    "    # Batch the audio features into groups of 50, which is the max number you can query spotify at once\n",
    "    song_list = divide_chunks(result_data, CHUNK_SIZE)\n",
    "    result_features = []\n",
    "    for sublist in song_list:\n",
    "        sublist = list(map(lambda d: d['id'], sublist))\n",
    "        # Add the audio features, remove unneeded one\n",
    "        \"\"\"\n",
    "        Here are the features:\n",
    "        dict_keys(['danceability', 'energy', 'key', 'loudness', 'mode',\n",
    "                   'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n",
    "                   'valence', 'tempo', 'type', 'id', 'uri', 'track_href',\n",
    "                   'analysis_url', 'duration_ms', 'time_signature'])\"\"\"\n",
    "        features = sp.audio_features(sublist)\n",
    "        features = list(map(del_unneeded, features))\n",
    "        result_features += features\n",
    "    \n",
    "    # merge the features into the other song data\n",
    "    result = list(zip(result_data, result_features))\n",
    "    result = list(map(lambda a: combine_fn(a[0], a[1]), result))\n",
    "    \n",
    "    return pd.DataFrame(result).set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'the business of emotion (feat. white sea)', 'id': '2TKso10B9HcIWy7HR1oP2g', 'artist': 'big data featuring white sea', 'popularity': 39}\n"
     ]
    }
   ],
   "source": [
    "songs = [\n",
    "    {'name': 'the business of emotion', 'artist': \"big data featuring white sea\", 'date': '2000-12-23'}\n",
    "]\n",
    "\"\"\"\n",
    "    {'name': 'Independent Women Part I', 'artist': \"Destiny's Child\", 'date': '2000-12-23'}, not in\n",
    "    {'name': 'He Loves U Not', 'artist': 'Dream', 'date': '2000-12-30'}, not in\n",
    "    {'name': 'She Misses Him', 'artist': 'Tim Rushlow', 'date': '2000-12-30'}, not in\n",
    "    {'name': 'Get Crunked Up', 'artist': 'Iconz Featuring Tony Manshino', 'date': '2000-12-30'} 69\n",
    "    \n",
    "    {'name': 'NAStradamus', 'artist': 'Nas', 'date': '2000-12-30'}\n",
    "    {'name': 'Independent Women Part I', 'artist': \"Destiny's Child\", 'date': '2000-12-30'},\n",
    "    {'name': 'Case Of The Ex (Whatcha Gonna Do)', 'artist': 'Mya', 'date': '2000-12-23'},\n",
    "    {'name': \"It Wasn't Me\", 'artist': 'Shaggy Featuring Ricardo \"RikRok\" Ducent', 'date': '2000-12-30'},\n",
    "    {'name': \"It Wasn't Me\", 'artist': 'Shaggy Featuring Ricardo \"RikRok\" Ducent', 'date': '2000-12-23'},\n",
    "    {'name': 'Case Of The Ex (Whatcha Gonna Do)', 'artist': 'Mya', 'date': '2000-12-23'},\n",
    "    {'name': 'I Wish', 'artist': 'R. Kelly', 'date': '??daf'}\n",
    "\"\"\"\n",
    "#Could not find 'fiesta' by 'r. kelly featuring jay-z' in top 20 results\n",
    "\n",
    "#x = convert_all_songs(songs)\n",
    "#x = get_song('NAStradamus', 'Nas')\n",
    "x = get_song(songs[0]['name'], songs[0]['artist'])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now that we have this functionality, lets run it on our data of top songs to create the end dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27400\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rockResults', 'rb') as f:\n",
    "    top_songs = pickle.load(f)\n",
    "\n",
    "top_songs = [song for year in top_songs for song in year] \n",
    "\n",
    "print(len(top_songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 'life can't get much better' by 'good charlotte' in top 20 results\n",
      "Could not find 'f**k with myself' by 'banks' in top 20 results\n",
      "Could not find 'in the dark' by '3 doors down' in top 20 results\n",
      "98.93167304992676\n",
      "Unique unfound: 3, total unfound: 187\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "result = convert_all_songs(top_songs)\n",
    "print(time.time() - start)\n",
    "print(\"Unique unfound: %s, total unfound: %s\" % (unique_unfound, total_unfound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rock.df', 'wb') as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27213\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006824817518248175"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "187/27400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
